I"(<p>attention 기법을 처음 적용하여 긴 문장의 NMT(Neural machine translation)에서 성능을 향상시킨 <a href="https://arxiv.org/pdf/1508.04025.pdf">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>을 리뷰를 해보겠다.</p>

<p><img src="http://localhost:4000/images/Effective_Approaches_to_Attention-based_Neural_Machine_Translation1.JPG" alt="" /><br />
<code>WMT</code> 그리고 비슷한 시대에 나온 <code>Jeans et al.</code> 논문에서 나온 <code>BLEU Score</code>보다 훨씬 좋은 성능을 보여 <code>State of Arts</code>를 달성했다.</p>

<p><a href="https://arxiv.org/pdf/1508.04025.pdf">논문</a>를 참고하자.</p>

<h2 id="abstract-introduction-요약">ABSTRACT, Introduction 요약</h2>

<p><code>attentional</code> 메커니즘은 최근 <code>Neural machine translation</code> 이하 <code>NMT</code>을 개선하는데 사용되었다. 그러나 <code>attentional</code>기반 <code>NMT</code>에서 유용한 구조를 탐구하는 작업은 거의 없었다. 본 논문은 모든 것을 확인하는 <code>global approach</code>과 일부만 보는 <code>local approach</code>라는 두 가지 <code>attentional</code> 메커니즘의 간단하고 효과적인 분류를 연구한다.</p>

<p>최근에 제안된 모델들은 <code>encoder–decoder</code>구조에 속한다. <code>encoder–decoder</code>방법은 <code>encoder neural network</code>로 소스 문장을 <code>fixed-length vector</code>로 인코딩 한다. 그런 다음 디코더는 인코딩된 <code>fixed-length vector</code>을 통해서 번역을 진행한다. 상식적으로 작은 <code>fixed-length vector</code> 하나로는 소스 문장이 길면 길수록 문장의 정보를 담기 힘들것이다. 실제로 입력 문장의 길이가 증가함에 따라 <code>encoder–decoder</code>의 성능이 급속히 저하된다.</p>

<p>이 문제를 해결하기 위해 모델이 자동으로 <code>Soft-alignments</code> 이하 <code>attention</code>작업을 진행한다.</p>

<h3 id="soft-alignmentattention-vs-hard-alignment">Soft-alignment(attention) VS Hard-alignment</h3>
<ul>
  <li><code>Soft-alignment(attention)</code> : 영어 -&gt; 한글 번역으로 예를 들자면 ‘I’가 ‘나’라는 정보를 컴퓨터 스스로 학습하는 것이다.</li>
  <li><code>Hard-alignment</code> : 예를 들면 위에 ‘I’가 ‘나’라는 정보를 사람이 직접 입력하는 것이다.</li>
</ul>

<p>이 접근법의 가장 중요한 특징 중 하나는 전체 입력 문장을 하나의 <code>fixed-length vector</code>로 인코딩하려고 시도하지 않는다는 것이다. 대신에, 디코딩을 진행할 때마다 번역된 단어가 위와 같이 관련된 문장을 찾는다. 우리는 이 방법으로 모델이 긴 문장에 더 잘 대처할 수 있게 한다는 것을 보여준다.</p>

<h2 id="3-learning-to-align-and-translate">3 LEARNING TO ALIGN AND TRANSLATE</h2>
<p>이 절에서 이 논문에 모델을 이야기한다. 인코더에서 <code>양방향 RNN</code>을 사용한다. 따라서 번역을 할 때 이전에 나온 단어 뿐만 아니라 다음에 나올 단어들까지 보면서 정확도를 높이겠다는 의미를 가진다. 디코딩 과정에서 위에서 설명한 <code>attention</code>을 통해 번역할 문장에 해당하는 소스문장을 찾는다. 아래 그림으로 전체구조를 볼 수 있다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE2.JPG" alt="" /></p>

<h3 id="31-decoder-general-description">3.1 DECODER: GENERAL DESCRIPTION</h3>
<p>이 구조에서 새로운 조건부 확률을 정의한다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE3.JPG" alt="" /></p>

<p><code>si</code>는 시간 <code>i</code>에 대한 <code>RNN</code>의 <code>hidden state</code>로 다음과 같이 계산된다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE4.JPG" alt="" /></p>

<p>이 식은 <code>RNN 과정</code>을 통해서 이전에 나온 것들과 <code>ci</code>부분을 통해서 다음번 단어를 생성한다는 의미이다.<br />
여기서 주목할만한 점은 <code>ci</code> 부분이다. 나머지 부분은 일반적인 <code>RNN</code> 구조를 가지지만 <code>ci</code>는 <code>attention</code> 과정을 의미한다. 계산 방법은 다음과 같다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE5.JPG" alt="" /></p>

<p><code>ci</code>는 <code>hj</code>와 <code>알파</code>의 곱으로 쓰인다. <code>hj</code>는 입력 시퀀스의 <code>j</code> 번째 단어라고 생각하면 된다.</p>

<p><code>알파</code>를 계산하는 방법은 다음과 같다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE6.JPG" alt="" /></p>

<p><code>알파</code>는 소프트맥스를 통해 <code>eij</code>에 대한 확률값을 가진다. 
<code>e</code>값은 다음과 같이 계산한다. <code>a</code>함수는 <code>alignment model</code>로 특정 j번째 입력 <code>hidden starte</code>와 직전에 만들어낸 단어의 <code>hidden starte</code> 사이의 관계값이다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE7.JPG" alt="" /></p>

<p><code>a</code>함수를 풀어쓰면 위와 같은 식이 된다. 출력 네트워크의 <code>hidden starte</code>와 인코더 네트워크의 <code>hidden starte</code>를 합치고 <code>tanh</code>를 진행한다. 즉 이것도 하나의 <code>뉴런네트워크</code>이다. <code>Vhj</code>는 이미 계산된 값이며 <code>Wsi-1</code> 값만 계속해서 계산하면 된다.</p>

<p>여기서 <code>i</code>는 변수로 출력하는 <code>word</code>의 <code>index</code>를 나타낸다. 즉 <code>word</code>를 만들 때마다 새롭게 계산된다. 그러므로 긴 문장이어도 <code>attention</code> 기법을 통해서 <code>attention</code> 기법을 사용하지 않는 방식보다 훨씬 높은 정확도를 가진다.</p>

<h3 id="32-encoder-bidirectional-rnn-for-annotating-sequences">3.2 ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES</h3>
<p><code>일반 RNN</code>은 <code>input sequence x</code>를 순방향으로만 계산한다. 번역 등을 할때는 이전 단어뿐만 아니라 다음 단어도 학습하길 원한다. 그래서 역방향까지 학습하는 <code>양방향 RNN</code>을 사용한다. 그리고 최근 음성 인식 쪽에서도 성공적으로 적용되었다고 한다. 참고 <a href="https://arxiv.org/pdf/1308.0850.pdf">(see, e.g., Graves et al., 2013).</a></p>

<p><code>양방향 RNN</code>은 순방향과 역방향 2개의 <code>RNN</code>으로 구성된다. 순방향은 <code>1,2,3,...,T</code> 까지 역방향은 <code>T,....,2,1</code>로 계산한다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE8.JPG" alt="" /></p>

<p>그리고 순방향과 역방향을 <code>concatenating</code>하여 <code>hj</code>를 구한다.</p>

<p>이 방법을 통해서 <code>hj</code>는 <code>j</code>번째 단어 앞뒤의 정보를 모두 알 수 있게 된다.</p>

<h2 id="4-experiment-settings">4 EXPERIMENT SETTINGS</h2>

<p>대표적으로 <code>English-French</code>번역을 가지고 테스트를 했다. <code>348M</code>개의 데이터로 테스트를 했다.</p>

<p><code>Hidden units</code>를 1000개로 계산한다. 그런데 이 모델을 <code>양방향 RNN</code>으로 순방향 1000개 역방향 1000개로 <code>일반 RNN</code>보다 좀 더 많은 <code>Hidden units</code>를 가지고 있다.</p>

<p><code>SGD</code>와 <code>Adadelta</code>를 사용했다. 각 <code>SGD</code> 업데이트 방향은 <code>80 sentences</code>의 <code>minibatch</code>를 사용하여 계산했다.</p>

<h3 id="성능">성능</h3>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE1.JPG" alt="" /></p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE9.JPG" alt="" /></p>

<p>처음에 보여준 그림과 같이 <code>RNNsearch</code>가 해당 모델이고, <code>RNNenc</code>가 이전 모델이다. 큰 성능 향상을 보였고 문장의 길이가 길어져도 급감하는 현상이 적어졌다. 그리고 트레이닝을 <code>50개</code>짜리로 더 많이 했을 때 더 높은 성능을 보여준다. <code>*</code>이 붙은 게 매우 오랫동안 학습을 한 경우인데, <code>No UNK</code> 즉 모르는 단어를 제외하면 <code>open source smt</code>로 유명한 <code>Moses</code>보다 높은 성능을 보여준다. 간단한 모델을 통해서 <code>Moses</code>보다 높은 성능을 발휘하는 것은 대단한 성과이다.</p>

<h2 id="5-results">5 RESULTS</h2>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE10.JPG" alt="" /></p>

<p>위 그림은 <code>x축</code>으로 <code>입력 sentences</code>, <code>y축</code>으로는 <code>출력 sentences</code>를 차례대로 썼고 각 네모의 밝기가 <code>알파</code> 값이다. 즉 밝을수록 관련이 높은 것이다. 불어와 는 일반적으로 어순이 비슷하여 무난하게 가기도 하지만 어떨 때는 어순이 거꾸로 일 때도 있다. 그리고 여러 정관사도 <code>1:n</code>으로 있을 수도 있는데 이럴 때도 <code>attention</code>이 해당하는 부분을 잘 처리하는 것을 그림을 통해 볼 수 있다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE11.JPG" alt="" /></p>

<p>일반적인 <code>seq2seq</code>모델의 문제점인 초반에는 번역이 잘 됐다가 문장이 길어지면 해석이 잘 안 되는 경향이 있는데 <code>attention</code>을 적용하여 긴 문장도 해석이 잘 된다고 한다.</p>

<p><img src="http://localhost:4000/images/NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNING_TO_ALIGN_AND_TRANSLATE12.JPG" alt="" /><br />
(구글 번역기를 돌려보니 뭔가 잘 되는 거 같다.)</p>
:ET